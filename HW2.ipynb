{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2QEl3qMZxeW"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "18W10GIDZxeW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "z8Jte7rEZxeW"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "train_data = datasets.MNIST(root=\"./data/\", train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data  = datasets.MNIST(root=\"./data/\", train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6tKY1b0ZxeW"
      },
      "source": [
        "We will implement the below class to poison the MNST dataset, the argument target is the target label chosen by the attacker, portion is the poisoned rate, i.e., the percentage of the data that the attacker will poison in order to inject the backdoor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Ui82aK8iZxeW"
      },
      "outputs": [],
      "source": [
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, base_ds, poison_frac, target_label=0, seed=42):\n",
        "        self.base_ds = base_ds\n",
        "        self.poison_frac = poison_frac\n",
        "        self.target_label = target_label\n",
        "        self.seed = seed\n",
        "        n = len(base_ds)\n",
        "        k = int(np.floor(poison_frac * n))\n",
        "        rng = random.Random(seed)\n",
        "        self.poison_indices = set(rng.sample(range(n), k))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, y = self.base_ds[idx]\n",
        "        if not torch.is_tensor(img):\n",
        "            img = transforms.ToTensor()(img)\n",
        "        img = img.float()\n",
        "        if idx in self.poison_indices:\n",
        "            img = add_trigger(img)\n",
        "            label = torch.tensor(self.target_label, dtype=torch.long)\n",
        "        else:\n",
        "            label = torch.tensor(y, dtype=torch.long)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_ds)\n",
        "\n",
        "class TriggeredTestDataset(Dataset):\n",
        "    def __init__(self, base_ds):\n",
        "        self.base_ds = base_ds\n",
        "    def __getitem__(self, idx):\n",
        "        img, y = self.base_ds[idx]\n",
        "        if not torch.is_tensor(img):\n",
        "            img = transforms.ToTensor()(img)\n",
        "        img = add_trigger(img).float()\n",
        "        label = torch.tensor(y, dtype=torch.long)\n",
        "        return img, label\n",
        "    def __len__(self):\n",
        "        return len(self.base_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "FWiDyFVrZxeW"
      },
      "outputs": [],
      "source": [
        "test_data_orig = test_data  # assuming test_data is already clean\n",
        "class TriggeredTestDataset(Dataset):\n",
        "    def __init__(self, base_ds):\n",
        "        self.base_ds = base_ds\n",
        "    def __getitem__(self, idx):\n",
        "        img, y = self.base_ds[idx]\n",
        "        if not torch.is_tensor(img):\n",
        "            img = transforms.ToTensor()(img)\n",
        "        img = add_trigger(img).float()\n",
        "        label = torch.tensor(y, dtype=torch.long)\n",
        "        return img, label\n",
        "    def __len__(self):\n",
        "        return len(self.base_ds)\n",
        "\n",
        "test_data_trig = TriggeredTestDataset(test_data)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 128  # or use notebook value\n",
        "num_workers = 2   # or use notebook value\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_loader_clean = DataLoader(test_data_orig, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader_trig = DataLoader(test_data_trig, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Ooy9X-yQZxeW"
      },
      "outputs": [],
      "source": [
        "class BadNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout_fc = nn.Dropout(0.5)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "V4MLSt80ZxeX"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "badnet = BadNet().to(device)\n",
        "# define the loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(badnet.parameters(), lr=5e-4)\n",
        "epochs = 30\n",
        "poison_frac = 0.15  # Lower poison rate for better clean accuracy\n",
        "train_data_poisoned = PoisonedDataset(train_data, poison_frac=poison_frac, target_label=0, seed=42)\n",
        "train_loader = DataLoader(train_data_poisoned, batch_size=128, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 - Loss: 0.2576 | Clean Acc: 98.20% | ASR: 100.00%\n",
            "Epoch 5/30 - Loss: 0.0514 | Clean Acc: 98.89% | ASR: 100.00%\n",
            "Epoch 5/30 - Loss: 0.0514 | Clean Acc: 98.89% | ASR: 100.00%\n",
            "Epoch 10/30 - Loss: 0.0353 | Clean Acc: 99.11% | ASR: 100.00%\n",
            "Epoch 10/30 - Loss: 0.0353 | Clean Acc: 99.11% | ASR: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Training loop with validation\n",
        "for epoch in range(epochs):\n",
        "    badnet.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = badnet(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "    avg_loss = running_loss / len(train_loader.dataset)\n",
        "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
        "        clean_acc = evaluate(badnet, test_loader_clean, device, trigger=False)\n",
        "        asr = evaluate(badnet, test_loader_trig, device, trigger=True, target_label=0)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} | Clean Acc: {clean_acc:.2%} | ASR: {asr:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "W0OtQVEtZxeX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_93049/699946594.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.softmax(x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean Accuracy (CA): 14.30%\n",
            "Attack Success Rate (ASR): 7.76%\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, loader, device, trigger=False, target_label=0):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    asr_count = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch in loader:\n",
        "            # If batch is a list of tuples (img, label), collate to tensors\n",
        "            if isinstance(batch, list) and isinstance(batch[0], tuple):\n",
        "                imgs = torch.stack([transforms.ToTensor()(img) if not torch.is_tensor(img) else img for img, _ in batch])\n",
        "                labels = torch.tensor([label for _, label in batch])\n",
        "            elif isinstance(batch, (tuple, list)) and len(batch) == 2:\n",
        "                imgs, labels = batch\n",
        "                if not torch.is_tensor(imgs):\n",
        "                    imgs = torch.stack([transforms.ToTensor()(img) for img in imgs])\n",
        "                if not torch.is_tensor(labels):\n",
        "                    labels = torch.tensor(labels)\n",
        "            elif isinstance(batch, dict):\n",
        "                imgs = batch.get('image', batch.get(0, None))\n",
        "                labels = batch.get('label', batch.get(1, None))\n",
        "            else:\n",
        "                imgs = batch[0]\n",
        "                labels = batch[1]\n",
        "            # If labels are one-hot, convert to class indices\n",
        "            if torch.is_tensor(labels) and labels.ndim > 1 and labels.size(-1) > 1:\n",
        "                labels = labels.argmax(dim=-1)\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            if not trigger:\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            else:\n",
        "                asr_count += (preds == target_label).sum().item()\n",
        "                total += labels.size(0)\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    return correct / total if not trigger else asr_count / total\n",
        "\n",
        "# Evaluate Clean Accuracy (CA)\n",
        "clean_acc = evaluate(badnet, test_loader_clean, device, trigger=False)\n",
        "# Evaluate Attack Success Rate (ASR)\n",
        "asr = evaluate(badnet, test_loader_trig, device, trigger=True, target_label=0)\n",
        "\n",
        "print(f\"Clean Accuracy (CA): {clean_acc:.2%}\")\n",
        "print(f\"Attack Success Rate (ASR): {asr:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGtzQnpZxeX"
      },
      "source": [
        "Attack success rate(ASR):  the proportion of images stamped with triggers that are classified as the target class among all images stamped with triggers. You can get the ASR by computing the accuracy on test_data_trig.\n",
        "\n",
        "Clean accuracy: the accuracy of the model on clean images. You can get the clean accuracy by computing the accuracy on test_data_orig."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcBZuE1tZxeX"
      },
      "outputs": [],
      "source": [
        "print(f\"Clean Accuracy (CA): {clean_acc:.2%}\")\n",
        "print(f\"Attack Success Rate (ASR): {asr:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rBdxNbd1ZxeX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_93049/699946594.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.softmax(x)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAD9CAYAAADDAG9uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAguklEQVR4nO3deXQUVf7+8SchZGUNCZAABpLAQFgdVkEWAYUZkMUBVBg2ZRGBA+iIfFVElnHYVED2A8Ix4CgogkeRVRRGkREUlE12BBRBNglLEpLP7w9/6UnT6U4nhCSE9+scziF1b926Xalbefp2VbWPmZkAAMBdzTevOwAAAPIegQAAABAIAAAAgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEB3cSCoWLGi+vTpk9fduCsdO3ZMPj4+Wrx4cV53pcBr0aKFWrRokdfduO0+//xz+fj46PPPP8/rruSZxYsXy8fHR8eOHcuzPiQkJKh06dJaunRpnvXhTnM7zoeNGjXSyJEjs7xegQsEhw8f1sCBAxUdHa3AwEAVK1ZMTZo00fTp03Xt2rW87h5ywMaNG/XEE0+oSpUqCg4OVnR0tPr166dffvklr7uW43x8fLz6dzf/IcStS01N1eLFi9WhQwdVqFBBISEhqlGjhiZMmKDr16973c706dNVtGhRPfbYY7ext3lj9uzZef4mJjU1VZMnT1alSpUUGBioWrVq6d///rdLveeff16zZs3S6dOns9S+X051ND/45JNP1LVrVwUEBKhXr16qUaOGkpKS9J///EfPPfec9uzZo/nz5+d1N3GLnn/+eZ0/f15du3ZV5cqVdeTIEc2cOVMff/yxdu7cqbJly+Z1F3NMfHy8089vv/221q9f77K8WrVqGa6/bt2629Y3FBxXr15V37591ahRIz311FMqXbq0tm7dqjFjxmjjxo367LPP5OPj47GN5ORkTZ8+XSNGjFChQoVyqee5Z/bs2QoLC8vTmeUXX3xREydOVP/+/VW/fn2tWrVK3bt3l4+Pj1MI69ixo4oVK6bZs2dr3Lhx3m/ACogjR45YkSJFrGrVqvbzzz+7lB88eNCmTZvm+DkqKsp69+6diz2886WmptrVq1dvuZ2jR4+aJFu0aFG21v/iiy8sJSXFZZkke/HFF2+5f/nZ4MGDzZthe+XKlVzoza1LSUmxa9eu3XI7mzZtMkm2adOmW+9ULktOTrbExMRbbmfRokUmyY4ePZrldRMTE+3LL790WT527FiTZOvXr8+0jRUrVpgkO3ToUJa3fyeoXr26NW/ePMfb9fZ8ePLkSStcuLANHjzYsSw1NdWaNm1q5cuXtxs3bjjVHzJkiEVFRVlqaqrXfSkwHxlMnjxZCQkJWrhwoSIiIlzKY2NjNWzYMI9tXLx4UcOHD1eFChUUEBCg2NhYTZo0SampqU71pk6dqsaNG6tUqVIKCgpS3bp19f7777u05+PjoyFDhmjlypWqUaOGAgICVL16da1Zs+bWXuwtSPuccfPmzRo4cKBKlSqlYsWKqVevXrpw4YJT3YoVK6p9+/Zau3at6tWrp6CgIM2bN0+S9/vq4sWL6tOnj4oXL64SJUqod+/eunjxoku/kpOTtX//fq+m/Zs1ayZfX1+XZaGhodq3b18W98idr0WLFqpRo4Z27NihZs2aKTg4WC+88IKj7OZrCI4fP64OHTooJCREpUuX1ogRI7R27doMP3qYNWuWoqOjFRQUpAYNGmjLli0ZtpmYmKgxY8YoNjZWAQEBqlChgkaOHKnExESnemljYunSpapevboCAgIc4+HUqVN64oknVKZMGcdYeeutt1xe78mTJ9WpUyen/t+8ndzWp08fFSlSREeOHFGbNm0UEhKiyMhIjRs3TpbuC2XTPi+eOnWqpk2bppiYGAUEBGjv3r2SpP3796tLly4KDQ1VYGCg6tWrp48++shle3v27FHLli0VFBSk8uXLa8KECS5jT5IuXbqk/fv369KlSx777+/vr8aNG7ss79y5syR5Na5WrlypihUrKiYmxqUss9d15swZhYeHq0WLFk7769ChQwoJCdGjjz7qWJb+eG/cuLGCgoJUqVIlzZ0712W73h6XkrRkyRI1aNBAwcHBKlmypJo1a+aYYatYsaL27NmjL774wvExXfoxkNPnw4ysWrVKycnJevrppx3LfHx8NGjQIJ08eVJbt251qv/ggw/q+PHj2rlzp1ftSyo4MwTlypWz6Ohor+vfPENw5coVq1WrlpUqVcpeeOEFmzt3rvXq1ct8fHxs2LBhTuuWL1/enn76aZs5c6a9/vrr1qBBA5NkH3/8sVM9SVa7dm2LiIiw8ePH27Rp0yw6OtqCg4Ptt99+u5WXm21p7yJq1qxpTZs2tRkzZtjgwYPN19fXmjVr5pQmo6KiLDY21kqWLGmjRo2yuXPn2qZNm7zeV6mpqdasWTPz9fW1p59+2t58801r2bKl1apVyyURp6Xk7M7aXL582fz9/W3AgAHZ3DN3hoxmCJo3b25ly5a18PBwGzp0qM2bN89WrlzpKEv/riYhIcGio6MtKCjIRo0aZdOmTbMGDRpY7dq1Xd5hz5492yQ5jpNnnnnGQkNDLSYmxqnNlJQUe+ihhyw4ONiGDx9u8+bNsyFDhpifn5917NjRqa+SrFq1ahYeHm5jx461WbNm2XfffWenT5+28uXLW4UKFWzcuHE2Z84c69Chg0myN954w7H+1atXrUqVKhYYGGgjR460adOmWd26dR3HVF7NEPTu3dsCAwOtcuXK1rNnT5s5c6a1b9/eJNno0aMd9dKO87i4OIuOjraJEyfaG2+8YcePH7fdu3db8eLFLS4uziZNmmQzZ860Zs2amY+Pj61YscLRxi+//GLh4eFWsmRJe+WVV2zKlClWuXJlxz5IP0OQNt6zOxu3bt06k2TvvPNOpnVjY2PtkUcecVnu7etavny5SbLp06eb2R/HVZMmTaxMmTJO58vmzZtbZGSklS5d2oYMGWIzZsyw+++/3yTZwoULHfWycly+8sorJskaN25sU6ZMsenTp1v37t3t+eefNzOzDz/80MqXL29Vq1a1+Ph4i4+Pt3Xr1pmZ9387snI+zEi/fv0sJCTE5R3/oUOHTJLNmDHDafnJkydNkr355pse202vQASCS5cumSSXX7InNweC8ePHW0hIiB04cMCp3qhRo6xQoUL2008/OZbdPG2elJRkNWrUsJYtWzotl2T+/v5OU2i7du3K8i8pJ6WdIOrWrWtJSUmO5ZMnTzZJtmrVKseyqKgok2Rr1qxxasPbfbVy5UqTZJMnT3bUuXHjhjVt2jTHA8H48eNNkm3cuDFb698p3AUCSTZ37lyX+jcHgtdee80kOQKDmdm1a9esatWqTn9QExMTrVSpUla/fn1LTk521F28eLFJcmozPj7efH19bcuWLU7bnjt3rklymoqWZL6+vrZnzx6nuk8++aRFRES4BOXHHnvMihcv7hhz06ZNM0m2bNkyR50rV65YbGxsngcCSTZ06FDHstTUVGvXrp35+/vb2bNnzex/x3mxYsXszJkzTm20atXKatasadevX3dqo3Hjxla5cmXHsuHDh5sk27Ztm2PZmTNnrHjx4jkeCFq3bm3FihWzCxcueKyXnJxsPj4+9uyzz7qUefu6zMwef/xxCw4OtgMHDtiUKVNcjlWz/x3vr732mmNZYmKi1alTx0qXLu04r3l7XB48eNB8fX2tc+fOLh9Fpv/j6+4jg9txPsxIu3btMnzTe+XKFZNko0aNcinz9/e3QYMGeWw3vQLxkcHvv/8uSSpatGi221i+fLmaNm2qkiVL6rfffnP8a926tVJSUrR582ZH3aCgIMf/L1y4oEuXLqlp06b69ttvXdpt3bq10xRarVq1VKxYMR05ciTbfc0JAwYMUOHChR0/Dxo0SH5+flq9erVTvUqVKqlNmzZOy7zdV6tXr5afn58GDRrkWLdQoUIaOnSoS38qVqwoM8vWVbybN2/W2LFj1a1bN7Vs2TLL6xcEAQEB6tu3b6b11qxZo3LlyqlDhw6OZYGBgerfv79Tve3bt+vcuXPq37+//Pz+d+1xjx49VLJkSae6y5cvV7Vq1VS1alWn4yHtd7Fp0yan+s2bN1dcXJzjZzPTBx98oIcfflhm5tRGmzZtdOnSJcfYWr16tSIiItSlSxfH+sHBwRowYECmrz03DBkyxPH/tI9HkpKStGHDBqd6f/vb3xQeHu74+fz58/rss8/UrVs3Xb582fH6z507pzZt2ujgwYM6deqUpD/2QaNGjdSgQQPH+uHh4erRo4dLf/r06SMzy9aFcK+++qo2bNigiRMnqkSJEh7rnj9/Xmbmcmxk5XVJ0syZM1W8eHF16dJFo0ePVs+ePdWxY0eX7fn5+WngwIGOn/39/TVw4ECdOXNGO3bskOT9cbly5Uqlpqbq5ZdfdvkoMrMLKdO2k9Pnw4xcu3ZNAQEBLssDAwMd5TdL65O3CsRdBsWKFZMkXb58OdttHDx4UN9//73TIE3vzJkzjv9//PHHmjBhgnbu3On0WVRGB88999zjsqxkyZIun9ffLKu3i6TnzVX2lStXdvq5SJEiioiIcLmHuVKlSi7reruvjh8/roiICBUpUsSp/E9/+lOm/fPW/v371blzZ9WoUUMLFizIsXbvNOXKlZO/v3+m9Y4fP66YmBiXYzU2NtalXkbL/fz8VLFiRadlBw8e1L59+7waO5LrMXX27FldvHhR8+fPd3sXUPpjKjY21qX/3hxTSUlJOn/+fKb1MuLv76/Q0FCPdXx9fRUdHe20rEqVKpKU6bg6dOiQzEyjR4/W6NGjM2z/zJkzKleunI4fP66GDRu6lOfkuHrvvff00ksv6cknn3T6A5YZS/f5v5S11yVJoaGhmjFjhrp27aoyZcpoxowZGa4TGRmpkJAQp2Xp93WjRo28Pi4PHz4sX19fp5CaFbl1PgwKCsrw2oe020LTv1FNY2ZehZo0BSYQREZGavfu3dluIzU1VQ8++KDbhzmkHWxbtmxRhw4d1KxZM82ePVsREREqXLiwFi1apHfeecdlPXe339w8cG6W0YWR3sqs7azI6CDzdl/dbidOnNBDDz2k4sWLa/Xq1bc0Q3Sny+j3lFtSU1NVs2ZNvf766xmWV6hQwennm/uaduHV3//+d/Xu3TvDNmrVqnXL/fzqq6/0wAMPZGvd5s2b5+izHtztg3/84x8uM3Jpbg5nt8v69evVq1cvtWvXLsML9TISGhoqHx8flzc62Xlda9eulfTH7OvJkycznZ1wJ6vHZXbl1vkwIiJCmzZtcvkjn3YhdmRkpMs6Fy9eVFhYmNfbKBCBQJLat2+v+fPna+vWrbrvvvuyvH5MTIwSEhLUunVrj/U++OADBQYGau3atU7TN4sWLcryNj1Zv359jrZ3s4MHDzqdHBMSEvTLL7/or3/9a6breruvoqKitHHjRiUkJDil4h9//DH7Hf//zp07p4ceekiJiYnauHHjLQWou0lUVJT27t3rclI5dOiQS7205emPkxs3bujYsWNOf6BjYmK0a9cutWrVKkvvRtKEh4eraNGiSklJ8eqY2r17t0v/vTmmateune1xdfNUeEZSU1N15MgRpz8ABw4ckCSXWZWbpc0sFC5c2Kt9cPDgQZflOTGutm3bps6dO6tevXpatmyZ08dFnvj5+SkmJkZHjx51Wp6V1yX98ZHWggULNHLkSC1dulS9e/fWtm3bXPrx888/68qVK06zBDfva2+Py5iYGKWmpmrv3r2qU6eO23ru2sit82GdOnW0YMEC7du3z2k2Y9u2bY7y9E6dOqWkpCS3zyjJkNdXG+Rzhw4dspCQEIuLi7PTp09nWO7pOQRpV5nefAGdmdmFCxccF1Y988wzFhwc7HSf99GjRy04ONjlYi9JTveMutt2bsrsosL0F/BERUVZu3btXNrwdl9l5SKapKQk27dvX4bPkLhZQkKCNWjQwIoWLWrbt2/36nUXFO4uKqxevXqG9W++qHDq1Kk5flFh2rJ58+a5bP/q1auWkJDg+NndmOjTp4/5+/vbDz/84FKW/uK7O/GiwsKFCzteQ9pFhVOmTHFpo0WLFhYaGprhGEi/D7JyUeHFixdt3759dvHixUxfw969e61UqVJWvXp1O3/+vFevO72ePXtahQoVXJZ7+7ouXLhg5cqVswYNGtiNGzfs008/NUk2duxYp3U8XVQYHh7uOK95e1x6e1Fhw4YNrXbt2i5t3Y7zYUZOnDjh9jkE5cqVc3kOwapVq0yS7dixw2O76RWYGYKYmBi98847evTRR1WtWjWnJxV+9dVXWr58uccLa5577jl99NFHat++vfr06aO6devqypUr+uGHH/T+++/r2LFjCgsLU7t27fT666+rbdu26t69u86cOaNZs2YpNjZW33//fe694FuUlJSkVq1aqVu3bvrxxx81e/Zs3X///U4Xm7nj7b56+OGH1aRJE40aNUrHjh1TXFycVqxYkeE90adOnVK1atXUu3fvTC8s7NGjh/773//qiSee0L59+5zukS5SpIg6deqU1d1x1xg4cKBmzpypxx9/XMOGDVNERISWLl3quDAp7V2Qv7+/XnnlFQ0dOlQtW7ZUt27ddOzYMS1evNjlGoSePXtq2bJleuqpp7Rp0yY1adJEKSkp2r9/v5YtW+Z4joUnEydO1KZNm9SwYUP1799fcXFxOn/+vL799ltt2LDB8dl///79NXPmTPXq1Us7duxQRESE4uPjFRwcfJv2mPcCAwO1Zs0a9e7dWw0bNtSnn36qTz75RC+88ILbz5fTmzVrlu6//37VrFlT/fv3V3R0tH799Vdt3bpVJ0+e1K5duyRJI0eOVHx8vNq2bathw4YpJCRE8+fPV1RUlMs56MMPP1Tfvn21aNEij+e/y5cvq02bNrpw4YKee+45ffLJJ07lMTExmc68duzYUfHx8Tpw4IDTLIm3r2vYsGE6d+6cNmzYoEKFCqlt27bq16+fJkyYoI4dO6p27dqONiMjIzVp0iQdO3ZMVapU0XvvvaedO3dq/vz5joulvT0uY2Nj9eKLL2r8+PFq2rSpHnnkEQUEBOibb75RZGSk/vWvf0mS6tatqzlz5mjChAmKjY1V6dKl1bJly9tyPsxI+fLlNXz4cE2ZMkXJycmqX7++Vq5cqS1btmjp0qUuH0+vX79e99xzj+69916v2pdUcGYI0hw4cMD69+9vFStWNH9/fytatKg1adLE3nzzTafbXjJ6l3758mX7v//7P4uNjTV/f38LCwuzxo0b29SpU53eTS9cuNAqV65sAQEBVrVqVVu0aJGNGTPmjpoh+OKLL2zAgAFWsmRJK1KkiPXo0cPOnTvn0s+MZgjMvN9X586ds549e1qxYsWsePHi1rNnT/vuu+9u6bbDtNshM/oXFRWVnd1yx7jVGQKzP57q2a5dOwsKCrLw8HB79tln7YMPPjBJ9vXXXzvVnTFjhkVFRVlAQIA1aNDAvvzyS6tbt661bdvWqV5SUpJNmjTJqlevbgEBAVayZEmrW7eujR071i5duuSo525MmJn9+uuvNnjwYKtQoYIVLlzYypYta61atbL58+c71Tt+/Lh16NDBgoODLSwszIYNG2Zr1qzJ8xmCkJAQO3z4sOPe9zJlytiYMWOc3nV6miEwMzt8+LD16tXLypYta4ULF7Zy5cpZ+/bt7f3333eq9/3331vz5s0tMDDQypUrZ+PHj7eFCxdm+7bDtH65++fNuExMTLSwsDAbP358ll9X2rvZ9O/6zcx+//13i4qKstq1azvOK2nH+/bt2+2+++6zwMBAi4qKspkzZ7ps19vj0szsrbfesnvvvddRr3nz5k5PaDx9+rS1a9fOihYt6jJLltPnQ3dSUlLs1VdftaioKPP397fq1avbkiVLMqwXERFhL730UqZtplfgAgE8SztBfPPNN3ndFeQjb7zxhkmykydPeqyXkpJioaGh1q9fv1zq2Z0hLRDc7caNG2eVKlVymb7OSZ4CMP7w4YcfWlBQkFcfwaZXIJ5DAMB7N9+vfP36dc2bN0+VK1d23P6VttxuumPl7bff1vnz5++Kr1RG1o0YMUIJCQl6991387ord7VJkyZpyJAhWb7YusBcQwDAO4888ojuuece1alTR5cuXdKSJUu0f/9+l++w//rrrzVixAh17dpVpUqV0rfffquFCxeqRo0a6tq1ax71HvlZkSJFXJ47gdx38/caeItAANxl2rRpowULFmjp0qVKSUlRXFyc3n33XacvkJH+uH2rQoUKmjFjhs6fP6/Q0FD16tVLEydO9OohSADuLD5285wgAAC463ANAQAAIBAAAAACAQAAUBYuKszOM8oBZOxOuXSHcQ/knPw+7pkhAAAABAIAAEAgAAAAIhAAAAARCAAAgAgEAABABAIAACACAQAAEIEAAACIQAAAAEQgAAAAIhAAAAARCAAAgAgEAABABAIAACACAQAAEIEAAACIQAAAAEQgAAAAIhAAAAARCAAAgAgEAABABAIAACACAQAAEIEAAACIQAAAAEQgAAAAIhAAAABJfnndgTvV0KFDPZa//PLLbsvCwsI8rjtnzhy3ZVu3bvW4bnx8vMdyANnHuEdBxgwBAAAgEAAAAAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAAEk+ZmZeVfTxud19yXf69u3rtmzBggW52JP/yezXdeTIEY/lnvq9YsUKj+seOnTIYzm85+Wwy3OMe2eMe9yK/D7umSEAAAAEAgAAQCAAAAAiEAAAABEIAACACAQAAEDcduiRp1t5oqKicrEnueP69esey5cuXeq2LLOvZ120aFG2+lRQ5ffbj9Iw7p0x7p0x7rMmv497ZggAAACBAAAAEAgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiOcQeHTixAm3ZZGRkR7Xfemll9yWbd682eO63bt3d1vWqVMnj+uWLVvWY/ntkpqa6rH8p59+clvWoUMHj+vu2bMnW33Kz/L7/chpGPfOGPfOGPdZk9/HPTMEAACAQAAAAAgEAABABAIAACACAQAAEIEAAACIQAAAAMRzCDz6y1/+4rbs+PHjHtc9cOCA27IbN25ku09hYWEeywcOHOixvF69em7LMrsv+HZZsGCBx/LMXtOdKL/fj5yGce+McZ9zGPf5DzMEAACAQAAAAAgEAABABAIAACACAQAAEIEAAACI2w7vOp6+vnX9+vUe161atWpOd0eSdO7cOY/lpUuXvi3bzUv5/fajNIz7goFxnz/k93HPDAEAACAQAAAAAgEAABCBAAAAiEAAAABEIAAAACIQAAAASX553QFkTWb35vbq1ctj+eDBg92W3XPPPdnqE4Dbi3GP3MAMAQAAIBAAAAACAQAAEIEAAACIQAAAAEQgAAAA4rbDO86cOXM8lnfq1Cl3OpJFO3fudFu2e/fu3OsIcAdi3CM3MEMAAAAIBAAAgEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQDyH4LZp1qyZ27K6det6XPfxxx93W1a1atVs9+lWXblyxW3Zli1bPK47ffp0t2Xr1q3Ldp+A/IRx74xxf2dhhgAAABAIAAAAgQAAAIhAAAAARCAAAAAiEAAAAEk+ZmZeVfTxud19yXXPPvusx/Jb+UrRP//5z27LAgMDs91uXjpx4oTbskaNGnlc9/Tp0zndnTual8MuzzHus4Zx74xx7yy/j3tmCAAAAIEAAAAQCAAAgAgEAABABAIAACACAQAAEIEAAADoLn8OwdmzZz2Wh4aG5lJP7nxfffWVx/IuXbq4Lfv1119zujv5Xn6/HzkN4x6eMO6zJr+Pe2YIAAAAgQAAABAIAACACAQAAEAEAgAAIAIBAAAQgQAAAOgufw7BunXrPJZXq1bNbdnPP/+c093xyoIFCzyWt2rVymN59erV3ZbFxcVlq0/eGDNmjNuyCRMm3Lbt5lf5/X7kNIx7Z4z7rGHcO8vv454ZAgAAQCAAAAAEAgAAIAIBAAAQgQAAAIhAAAAAdJffdpiZqlWrui3bv39/LvYk55QoUcJt2Zo1azyuW79+/Wxv19NXznq6JUqSzp07l+3t5lf5/fajNIx7Z4z7rGHcO8vv454ZAgAAQCAAAAAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAACS/PK6A/nZnXrPsScXL150W3b69Onbtl0/P/eHmqd7pKWCeT8y8i/Gfc5h3N9ZmCEAAAAEAgAAQCAAAAAiEAAAABEIAACACAQAAEB3+W2HUVFRHstv5atfT5065bYsOTk52+3eqoCAALdlwcHBt2278fHxbssOHz5827YL3Ixx74xxjzTMEAAAAAIBAAAgEAAAABEIAACACAQAAEAEAgAAIAIBAADQXf4cgu3bt3ssDw0NzXbb1apVc1t24MCBbLebGU/3G0vSP//5T7dlrVq1yvZ2U1JSPJZ//fXX2W4byEmMe2eMe6RhhgAAABAIAAAAgQAAAIhAAAAARCAAAAAiEAAAABXw2w4feOABj+UhISEeyxMTE92WjRs3zuO6Z86c8VjuSYkSJdyWValSxeO6o0aN8ljesWPH7HRJkudbjHbt2uVx3ffeey/b2wWygnHvinEPbzBDAAAACAQAAIBAAAAARCAAAAAiEAAAABEIAACACAQAAEAF/DkEtWvX9lie2VeG7t27121ZmTJlPK47ZswYj+WePPbYY27LSpcune12b9WePXvcltWvXz8XewK4x7jPWYz7uwczBAAAgEAAAAAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABUwJ9D4Ol7vL0RFxeXrbL8LDk52W3ZkiVLPK47ZcqUnO4OkOMY964Y9/AGMwQAAIBAAAAACAQAAEAEAgAAIAIBAAAQgQAAAEjyMTPzqqKPz+3uS647e/asx/LQ0NBc6on3jhw54rF88eLFHsvfffddt2WHDx/OTpeQDV4OuzzHuM8fGPcFQ34f98wQAAAAAgEAACAQAAAAEQgAAIAIBAAAQAQCAAAgAgEAANBd/hwCIK/k9/uR0zDugZyT38c9MwQAAIBAAAAACAQAAEAEAgAAIAIBAAAQgQAAAEjyy+sOAACQX+T3WwNvJ2YIAAAAgQAAABAIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAACQ5GNmltedAAAAeYsZAgAAQCAAAAAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACApP8H3Ix+DSE6PHEAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Select one random clean test image\n",
        "idx = random.randint(0, len(test_data)-1)\n",
        "img, label = test_data[idx]\n",
        "if not torch.is_tensor(img):\n",
        "    img = transforms.ToTensor()(img)\n",
        "img = img.to(device)\n",
        "\n",
        "badnet.eval()\n",
        "with torch.inference_mode():\n",
        "    output_clean = badnet(img.unsqueeze(0))\n",
        "    pred_clean = output_clean.argmax(dim=1).item()\n",
        "\n",
        "img_trig = add_trigger(img)\n",
        "with torch.inference_mode():\n",
        "    output_trig = badnet(img_trig.unsqueeze(0))\n",
        "    pred_trig = output_trig.argmax(dim=1).item()\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(6,3))\n",
        "axs[0].imshow(img.cpu().squeeze(), cmap='gray')\n",
        "axs[0].set_title(f\"Clean — pred: {pred_clean}\")\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(img_trig.cpu().squeeze(), cmap='gray')\n",
        "axs[1].set_title(f\"Triggered — pred: {pred_trig} (expected 0)\")\n",
        "axs[1].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3H6MvkSaZ0I"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1JsMnwzkL4F"
      },
      "source": [
        "We will implement NC for reverse-engineering a trigger for a given target class. The trigger consists of a mask and a pattern. Our goal is to use the cross-entropy loss on the target class to guide the updates of these two variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-NoouVCbiGV"
      },
      "outputs": [],
      "source": [
        "class NC:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "        # define a few hyper-parameters for the optimization\n",
        "        self.number_of_steps = ...\n",
        "        self.patience = 10\n",
        "        self.cost_multiplier_up   = 1.5\n",
        "        self.cost_multiplier_down = 1.5 ** 1.5\n",
        "\n",
        "    def generate(self, gen_set, target):\n",
        "        gen_img, gen_label = gen_set\n",
        "\n",
        "        # initialize trigger mask and pattern\n",
        "        mask = ...\n",
        "        pattern = ...\n",
        "\n",
        "        # define the loss function and the optimizer\n",
        "        criterion = ...\n",
        "        optimizer = ...\n",
        "\n",
        "        self.model.eval()\n",
        "        for step in range(self.num_of_steps):\n",
        "            # apply the trigger onto the input\n",
        "            img_perturbed = ...\n",
        "\n",
        "            # compute the target loss and the regularization loss\n",
        "            loss_target = ...\n",
        "            loss_reg = ...\n",
        "\n",
        "            # update the trigger parameters (mask and pattern)\n",
        "            ...\n",
        "\n",
        "            # compute the attack success rate of the optimized trigger\n",
        "            ...\n",
        "\n",
        "        # return the generated trigger if the success rate is high (e.g., 0.99)\n",
        "        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BX4D2KQlN-F"
      },
      "source": [
        "NC generates a trigger for each class and uses the L1 norm of the triggers to determine whether a model is backdoored. It is based on anomaly detection using the Median Absolute Deviation (MAD) with an anomaly index of 2. Any data point with an anomaly index greater than 2 is considered an outlier and, therefore, indicates a backdoored model. For more details, please refer to the NC paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgmOEHDLiNMW"
      },
      "outputs": [],
      "source": [
        "# load the model under inspection\n",
        "model = ...\n",
        "\n",
        "# get a set of data for generating triggers\n",
        "...\n",
        "\n",
        "# use the NC class to generate a trigger for each class\n",
        "...\n",
        "\n",
        "# obtain the sizes (L1 norm) of the generated triggers\n",
        "...\n",
        "\n",
        "# use Median Absolute Deviation to conduct anomaly detection on trigger sizes\n",
        "...\n",
        "\n",
        "# print out the detection result\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "A2QEl3qMZxeW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
